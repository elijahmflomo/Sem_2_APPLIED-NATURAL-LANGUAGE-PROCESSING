{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7Vl5D0AvWz7KDD80yG7eI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elijahmflomo/Sem_2_APPLIED-NATURAL-LANGUAGE-PROCESSING/blob/main/Lab_Assignment_1_Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario-Based Question 1: Customer Feedback Analysis\n",
        "\n",
        ">\n",
        "\n"
      ],
      "metadata": {
        "id": "az4uZlvZRLZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 0: Setup and Imports**\n",
        "\n",
        "*First, I need to import the tools and download the \"knowledge packs\" (corpora) that NLTK uses to identify words and grammar.*"
      ],
      "metadata": {
        "id": "gklvyyzAR67b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BN41bYqjST1G",
        "outputId": "7805a3d7-fa4d-4925-b06c-32d970e3c1d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tasks 1 & 2: Load Data and Normalize**\n",
        "\n",
        "*I will take the example review and a few others. I'll use Regular Expressions (Regex) to strip out punctuationâ€”this is much faster than manual loops.*"
      ],
      "metadata": {
        "id": "L7AbVlbLSiXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load sample set\n",
        "reviews = [\n",
        "    \"Loved the products!!! Delivery was sooo slow ðŸ˜Š\",\n",
        "    \"The item is amazing, but the packaging was damaged.\",\n",
        "    \"I'm not happy with the service. Won't buy again!\",\n",
        "    \"Fast delivery and great quality. Highly recommended.\"\n",
        "]\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Removing punctuation and special characters (keeping only letters and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "normalized_reviews = [normalize_text(r) for r in reviews]\n",
        "print(f\"Normalized: {normalized_reviews[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzjyBY5DSdcM",
        "outputId": "c060377b-f7c2-4439-b3fc-b8758f9cf040"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized: loved the products delivery was sooo slow \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tasks 3 & 4: Tokenization and Stop Word Removal**\n",
        "\n",
        "*Tokenization chops the sentence into individual words. Stop words are common words like \"the,\" \"is,\" and \"in\" that don't carry much sentiment meaning*"
      ],
      "metadata": {
        "id": "ynkH9jhgTUl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "processed_tokens = []\n",
        "for review in normalized_reviews:\n",
        "    # 3. Word Tokenization\n",
        "    tokens = word_tokenize(review)\n",
        "\n",
        "    # 4. Remove Stop Words\n",
        "    filtered_tokens = [w for w in tokens if w not in stop_words]\n",
        "    processed_tokens.append(filtered_tokens)\n",
        "\n",
        "print(f\"Tokens after stop word removal: {processed_tokens[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nABhmr-DTlqz",
        "outputId": "a8584e74-697c-4955-c941-a3bcbaeef1e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens after stop word removal: ['loved', 'products', 'delivery', 'sooo', 'slow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5: Stemming vs. Lemmatization**\n",
        "\n",
        "This is a crucial part of this lab.\n",
        "\n",
        "Stemming: A \"crude\" method that chops off the ends of words (e.g., \"slowly\" becomes \"slow\").\n",
        "\n",
        "Lemmatization: A \"smart\" method that uses a dictionary to find the root word (e.g., \"better\" becomes \"good\")."
      ],
      "metadata": {
        "id": "0NrsbXLvUGf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Let's compare them on a single review\n",
        "example_tokens = processed_tokens[0] # ['loved', 'products', 'delivery', 'sooo', 'slow']\n",
        "\n",
        "stemmed = [stemmer.stem(w) for w in example_tokens]\n",
        "lemmatized = [lemmatizer.lemmatize(w) for w in example_tokens]\n",
        "\n"
      ],
      "metadata": {
        "id": "Uw4Fv-WMUrr2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 6: Final Cleaned Output**\n",
        "\n",
        "In a real-world scenario, we usually prefer Lemmatization because it keeps the words readable. Here is a comparison table of how the data changed:"
      ],
      "metadata": {
        "id": "gRYtYc5dVSQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Final Cleaned Output ---\\n\")\n",
        "\n",
        "print(f\"Original:   {example_tokens}\")\n",
        "print(f\"Stemmed:    {stemmed}\")\n",
        "print(f\"Lemmatized: {lemmatized}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3S6lxT1Wnyt",
        "outputId": "cf1876b2-8973-48ed-d857-bbfc764f94d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Final Cleaned Output ---\n",
            "\n",
            "Original:   ['loved', 'products', 'delivery', 'sooo', 'slow']\n",
            "Stemmed:    ['love', 'product', 'deliveri', 'sooo', 'slow']\n",
            "Lemmatized: ['loved', 'product', 'delivery', 'sooo', 'slow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SkhfT377XlmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scenario-Based Question 2: News Article Classification\n",
        "System**"
      ],
      "metadata": {
        "id": "ZSA-nODyXmWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1: Read a Sample News Article**\n",
        "\n",
        "Here I'll simulate a real news article with dates, numbers, and formal language.\n",
        "\n"
      ],
      "metadata": {
        "id": "AwCFkGspZ3zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article = \"\"\"\n",
        "On March 15, 2024, the Government of India announced a new technology policy.\n",
        "The initiative aims to invest $5 billion in artificial intelligence and healthcare systems.\n",
        "Experts believe this move will boost economic growth by 10 percent over the next five years.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "NgMFhxRcaA3V"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2: Sentence Tokenization**\n",
        "\n",
        "Sentence tokenization splits text into sentences."
      ],
      "metadata": {
        "id": "cXgFO7HqbHa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(article)\n",
        "\n",
        "print(\"Sentences:\")\n",
        "for s in sentences:\n",
        "    print(s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlDJpGZTbNBT",
        "outputId": "10871308-0e28-40e5-ff11-adf90cdf7530"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "\n",
            "On March 15, 2024, the Government of India announced a new technology policy.\n",
            "The initiative aims to invest $5 billion in artificial intelligence and healthcare systems.\n",
            "Experts believe this move will boost economic growth by 10 percent over the next five years.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3: Word Tokenization**\n",
        "\n",
        "Word tokenization splits sentences into words."
      ],
      "metadata": {
        "id": "eWv_-tSqbS-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    words.extend(tokens)\n",
        "\n",
        "print(\"Word Tokens:\")\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hACN4toSbWQ-",
        "outputId": "6ac57fce-2574-47c0-9882-0c480e48b844"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens:\n",
            "['On', 'March', '15', ',', '2024', ',', 'the', 'Government', 'of', 'India', 'announced', 'a', 'new', 'technology', 'policy', '.', 'The', 'initiative', 'aims', 'to', 'invest', '$', '5', 'billion', 'in', 'artificial', 'intelligence', 'and', 'healthcare', 'systems', '.', 'Experts', 'believe', 'this', 'move', 'will', 'boost', 'economic', 'growth', 'by', '10', 'percent', 'over', 'the', 'next', 'five', 'years', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Normalization**\n",
        "\n",
        "handle:\n",
        "\n",
        "âœ” Lowercasing\n",
        "\n",
        "âœ” Dates â†’ <DATE>\n",
        "\n",
        "âœ” Numbers â†’ <NUM>\n",
        "\n",
        "âœ” Remove punctuation\n",
        "\n",
        "Why?\n",
        "\n",
        "ML models work better with consistent patterns."
      ],
      "metadata": {
        "id": "1cnlKU1kbtwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_words = []\n",
        "\n",
        "for word in words:\n",
        "    word = word.lower()\n",
        "\n",
        "    # Replace dates and numbers\n",
        "    if re.fullmatch(r'\\d+', word):\n",
        "        word = '<NUM>'\n",
        "    elif re.fullmatch(r'\\d{4}', word):\n",
        "        word = '<DATE>'\n",
        "\n",
        "    # Remove punctuation\n",
        "    word = re.sub(r'[^\\w<>]', '', word)\n",
        "\n",
        "    if word != '':\n",
        "        normalized_words.append(word)\n",
        "\n",
        "print(\"Normalized Words:\")\n",
        "print(normalized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Szc6i__gb9vz",
        "outputId": "d4ea2bcb-61c2-4df1-cbcb-2c373d8db290"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Words:\n",
            "['on', 'march', '<NUM>', '<NUM>', 'the', 'government', 'of', 'india', 'announced', 'a', 'new', 'technology', 'policy', 'the', 'initiative', 'aims', 'to', 'invest', '<NUM>', 'billion', 'in', 'artificial', 'intelligence', 'and', 'healthcare', 'systems', 'experts', 'believe', 'this', 'move', 'will', 'boost', 'economic', 'growth', 'by', '<NUM>', 'percent', 'over', 'the', 'next', 'five', 'years']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5: Remove Stop Words (Preserve Keywords)**\n",
        "\n",
        "*Remove common words without losing domain terms like technology, healthcare.*"
      ],
      "metadata": {
        "id": "LLmVgylUcYXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_words = [\n",
        "    word for word in normalized_words\n",
        "    if word not in stop_words\n",
        "]\n",
        "\n",
        "print(\"After Stop Word Removal:\")\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1Qb4sAlcmE1",
        "outputId": "5fbf558a-23f6-4e10-8908-6c2b380c0b53"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Stop Word Removal:\n",
            "['march', '<NUM>', '<NUM>', 'government', 'india', 'announced', 'new', 'technology', 'policy', 'initiative', 'aims', 'invest', '<NUM>', 'billion', 'artificial', 'intelligence', 'healthcare', 'systems', 'experts', 'believe', 'move', 'boost', 'economic', 'growth', '<NUM>', 'percent', 'next', 'five', 'years']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 6: Lemmatization**\n",
        "\n",
        "Lemmatization converts words to meaningful base forms."
      ],
      "metadata": {
        "id": "ox4imQfFctPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_words = [\n",
        "    lemmatizer.lemmatize(word)\n",
        "    for word in filtered_words\n",
        "]\n",
        "\n",
        "print(\"Lemmatized Words:\")\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0tsUF4KcwBY",
        "outputId": "904c6ab2-0536-427d-c8b5-8cef8589f517"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words:\n",
            "['march', '<NUM>', '<NUM>', 'government', 'india', 'announced', 'new', 'technology', 'policy', 'initiative', 'aim', 'invest', '<NUM>', 'billion', 'artificial', 'intelligence', 'healthcare', 'system', 'expert', 'believe', 'move', 'boost', 'economic', 'growth', '<NUM>', 'percent', 'next', 'five', 'year']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 7: Final Transformed Text (For Feature Extraction)**\n",
        "\n",
        "*We join cleaned words back into text.*"
      ],
      "metadata": {
        "id": "N6hwf_q1c327"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_text = \" \".join(lemmatized_words)\n",
        "\n",
        "print(\"Final Transformed Text:\")\n",
        "print(final_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j2-d-Tpc_A4",
        "outputId": "2631ed06-8792-42fe-e444-fa8f16967760"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Transformed Text:\n",
            "march <NUM> <NUM> government india announced new technology policy initiative aim invest <NUM> billion artificial intelligence healthcare system expert believe move boost economic growth <NUM> percent next five year\n"
          ]
        }
      ]
    }
  ]
}