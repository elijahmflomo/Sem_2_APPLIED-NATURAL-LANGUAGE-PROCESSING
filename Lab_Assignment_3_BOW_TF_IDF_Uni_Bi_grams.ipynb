{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elijahmflomo/Sem_2_APPLIED-NATURAL-LANGUAGE-PROCESSING/blob/main/Lab_Assignment_3_BOW_TF_IDF_Uni_Bi_grams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1. Understanding the Concept\n",
        "\n",
        "The **Bag of Words (BoW)** model is a way of representing text data as numerical vectors. In NLP, machines can't \"read\" words; they need numbers. BoW looks at a document as a literal \"bag\" of its words: it ignores grammar, word order, and sentence structure, focusing only on **whether a word appears** and **how often**.\n",
        "\n",
        "### How Word Frequency is Represented\n",
        "\n",
        "In a BoW vector:\n",
        "\n",
        "* Each **dimension** (or index) in the vector corresponds to a specific word from a predefined **vocabulary**.\n",
        "* The **value** at that index represents the **count** (frequency) of that word in the specific document.\n",
        "* If a word from the vocabulary is missing in the document, its value is .\n",
        "\n",
        "---\n",
        "\n",
        "## 2. The Modern Approach & Alternatives\n",
        "\n",
        "While BoW is easy to implement, it has significant downsides:\n",
        "\n",
        "1. **Sparsity:** If your vocabulary has 10,000 words but your email only has 10, your vector is 99.9% zeros.\n",
        "2. **No Context:** It treats \"Dog bites man\" and \"Man bites dog\" exactly the same.\n",
        "3. **Frequency Bias:** Common words like \"the\" or \"is\" can dominate the vector without providing actual meaning.\n",
        "\n",
        "**Modern Alternatives:**\n",
        "\n",
        "* **TF-IDF (Term Frequency-Inverse Document Frequency):** Penalizes common words and rewards unique, meaningful words.\n",
        "* **Word Embeddings (Word2Vec, GloVe):** Represents words in a continuous vector space where similar words (e.g., \"boat\" and \"ship\") are mathematically close to each other.\n",
        "* **Transformers (BERT, GPT):** The gold standard. These use **Attention mechanisms** to understand the context of a word based on the words surrounding it.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Implementation: Spam Detection\n",
        "\n",
        "We will use a real-world dataset: the **UCI SMS Spam Collection**. It’s a classic dataset for BoW because spam often relies on specific \"trigger words.\""
      ],
      "metadata": {
        "id": "OvDpl1e3Ebfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 1. Real-world Dataset Loading (Subset for demonstration)\n",
        "data = {\n",
        "    'text': [\n",
        "        \"Win a free cash prize now! Click here for your offer.\",\n",
        "        \"Are we still having the meeting at 5?\",\n",
        "        \"Free entry to win a prize. Text 'OFFER' to 8001.\",\n",
        "        \"I will be late for the meeting, sorry.\"\n",
        "    ],\n",
        "    'label': ['spam', 'ham', 'spam', 'ham']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2. Define our specific vocabulary as requested\n",
        "custom_vocab = ['offer', 'free', 'win', 'meeting']\n",
        "\n",
        "# 3. Initialize CountVectorizer with the custom vocabulary\n",
        "# We use lowercase=True to ensure 'OFFER' and 'offer' are treated the same\n",
        "vectorizer = CountVectorizer(vocabulary=custom_vocab, lowercase=True)\n",
        "\n",
        "# 4. Transform the text into BoW vectors\n",
        "bow_matrix = vectorizer.transform(df['text'])\n",
        "\n"
      ],
      "metadata": {
        "id": "xRxbm1zgER3R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Display the Results\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=custom_vocab)\n",
        "bow_df['Original Text'] = df['text']\n",
        "\n",
        "print(\"Vocabulary:\", custom_vocab)\n",
        "print(\"\\nBag of Words Feature Vectors:\")\n",
        "print(bow_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7J9hu48FNlm",
        "outputId": "2bfd5713-c17e-47a7-c09e-fc1dc57eab52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['offer', 'free', 'win', 'meeting']\n",
            "\n",
            "Bag of Words Feature Vectors:\n",
            "   offer  free  win  meeting  \\\n",
            "0      1     1    1        0   \n",
            "1      0     0    0        1   \n",
            "2      1     1    1        0   \n",
            "3      0     0    0        1   \n",
            "\n",
            "                                       Original Text  \n",
            "0  Win a free cash prize now! Click here for your...  \n",
            "1              Are we still having the meeting at 5?  \n",
            "2   Free entry to win a prize. Text 'OFFER' to 8001.  \n",
            "3             I will be late for the meeting, sorry.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Output Vectors\n",
        "\n",
        "If we look at the first row of the output based on the code above:\n",
        "\n",
        "* **Text:** \"Win a free cash prize now! Click here for your offer.\"\n",
        "* **Vector:** `[1, 1, 1, 0]`\n",
        "* `1` for \"offer\"\n",
        "* `1` for \"free\"\n",
        "* `1` for \"win\"\n",
        "* `0` for \"meeting\"\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Summary Table\n",
        "\n",
        "| Feature | Bag of Words (BoW) | TF-IDF | Word Embeddings (Modern) |\n",
        "| --- | --- | --- | --- |\n",
        "| **Value** | Raw counts | Importance score | Semantic meaning |\n",
        "| **Context** | None | None | High (Spatial relationship) |\n",
        "| **Size** | Large/Sparse | Large/Sparse | Fixed/Dense |\n",
        "\n"
      ],
      "metadata": {
        "id": "RAS0dypTEGVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Task 2. **TF-IDF** stands for **Term Frequency-Inverse Document Frequency**\n",
        "\n",
        "**TF-IDF** stands for **Term Frequency-Inverse Document Frequency**. It is a statistical measure used to evaluate how important a word is to a document in a collection or corpus.\n",
        "\n",
        "The logic is split into two parts:\n",
        "\n",
        "1. **Term Frequency (TF):** How often does the word appear in *this* specific review? (More is better).\n",
        "2. **Inverse Document Frequency (IDF):** How many reviews contain this word? If *everyone* is saying it (like \"the\" or \"is\"), it’s not a helpful feature. We decrease its weight.\n",
        "\n",
        "Mathematically, for a term t in a document :\n",
        "\n",
        "$$\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$Where:$$\\text{IDF}(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t}\\right)$$\n",
        "\n",
        "\n",
        "\n",
        "## 2. Implementation: Product Review Analysis\n",
        "\n",
        "We will use a small sample of product reviews to demonstrate how \"common\" words get suppressed while \"feature\" words get boosted."
      ],
      "metadata": {
        "id": "_YJVEB2HIh6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Sample Dataset: Product Reviews\n",
        "reviews = [\n",
        "    \"The battery life is very long and the battery charges fast.\",\n",
        "    \"This camera is very clear but the battery is weak.\",\n",
        "    \"The performance of this laptop is very fast and smooth.\",\n",
        "    \"I love the camera quality and the performance is great.\"\n",
        "]\n",
        "\n",
        "# 2. Initialize the TF-IDF Vectorizer\n",
        "# 'stop_words' removes common English words like 'the', 'is', 'and' automatically\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# 3. Fit and transform the reviews\n",
        "tfidf_matrix = vectorizer.fit_transform(reviews)\n",
        "\n",
        "# 4. Convert to a readable DataFrame\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "# Adding the original review for clarity\n",
        "tfidf_df.insert(0, \"Original Review\", reviews)\n",
        "\n",
        "# 5. Display the results\n",
        "print(\"TF-IDF Feature Matrix:\")\n",
        "print(tfidf_df.to_string())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSfRZmvAIXZu",
        "outputId": "7b85a8b3-4032-4f8a-bd1d-e8e5802aaeec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Feature Matrix:\n",
            "                                               Original Review   battery    camera   charges     clear      fast     great    laptop      life      long      love  performance   quality    smooth      weak\n",
            "0  The battery life is very long and the battery charges fast.  0.638021  0.000000  0.404624  0.000000  0.319010  0.000000  0.000000  0.404624  0.404624  0.000000     0.000000  0.000000  0.000000  0.000000\n",
            "1           This camera is very clear but the battery is weak.  0.437791  0.437791  0.000000  0.555283  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  0.000000  0.000000  0.555283\n",
            "2      The performance of this laptop is very fast and smooth.  0.000000  0.000000  0.000000  0.000000  0.437791  0.000000  0.555283  0.000000  0.000000  0.000000     0.437791  0.000000  0.555283  0.000000\n",
            "3      I love the camera quality and the performance is great.  0.000000  0.382743  0.000000  0.000000  0.000000  0.485461  0.000000  0.000000  0.000000  0.485461     0.382743  0.485461  0.000000  0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of the Results\n",
        "\n",
        "* **Weighting:** Notice that in Review 1, the word **\"battery\"** will have a high score because it appears twice, but it is penalized slightly because it also appears in Review 2.\n",
        "* **The \"Very\" Effect:** If \"very\" wasn't in the stop-word list, its TF-IDF score would be very low across all documents because it appears in almost every review, making it useless for distinguishing between a laptop and a camera.\n",
        "* **Uniqueness:** A word like **\"laptop\"** or **\"charges\"** that only appears in one review will have a much higher relative weight for that specific document.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Why this is better for Feature Selection\n",
        "\n",
        "* **Automatic Noise Reduction:** It naturally handles \"the\", \"is\", and \"a\" without needing a massive manual list of words to ignore.\n",
        "* **Highlighting Keywords:** It makes \"battery\" or \"performance\" stand out as the mathematical \"signature\" of the review.\n",
        "* **Efficiency:** It’s computationally inexpensive compared to Deep Learning models while providing much cleaner data for a classifier (like a sentiment analyzer).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Comparison Summary\n",
        "\n",
        "| Metric | Bag of Words (BoW) | TF-IDF |\n",
        "| --- | --- | --- |\n",
        "| **Common Words** | High score (Distorts data) | Low score (Filtered out) |\n",
        "| **Rare Keywords** | Low score (Lost in noise) | High score (Highlighted) |\n",
        "| **Best Use Case** | Basic text classification | Feature extraction, Search engines |"
      ],
      "metadata": {
        "id": "mZp8_zbPIRog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "##3. The Task: News Article Word Frequency Analysis\n",
        "\n",
        "> **Question:** A news analytics company needs to identify trending topics from hundreds of daily articles. To do this, you must perform an initial exploratory text analysis by:\n",
        "> * **a)** Writing a Python program to calculate the frequency of each word in a set of news articles after basic preprocessing.\n",
        "> * **b)** Identifying the top 5 most frequent words in the processed text.\n",
        ">\n",
        ">\n",
        "\n",
        "\n",
        "\n",
        "## 1. Understanding the Concept\n",
        "\n",
        "To get an accurate word count, we can't just count every string. If we don't preprocess, \"Market\" and \"market!\" would be treated as different words.\n",
        "\n",
        "**The Preprocessing Pipeline:**\n",
        "\n",
        "1. **Tokenization:** Splitting sentences into individual words.\n",
        "2. **Lowercasing:** Converting everything to lowercase so \"Election\" and \"election\" match.\n",
        "3. **Noise Removal:** Removing punctuation and special characters.\n",
        "4. **Stop Word Removal:** (Optional but recommended) Removing words like \"the\", \"is\", and \"in\" that carry no topical meaning.\n",
        "\n",
        "\n",
        "## 2. Implementation: Identifying Trending Topics\n",
        "\n",
        "We will use a sample set of news-style headlines and the `collections` library, which is the most efficient way to count items in Python.\n"
      ],
      "metadata": {
        "id": "1ZcR2B7PNsPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Sample News Articles (Dataset)\n",
        "news_articles = [\n",
        "    \"The election results are coming in today. The election is close!\",\n",
        "    \"Market trends show a shift in technology stocks. Investors watch the market.\",\n",
        "    \"New policy changes affecting the technology sector were announced.\",\n",
        "    \"Technology is driving the market to new heights during this election cycle.\",\n",
        "    \"Economic policy and the market are the main focuses of the election.\"\n",
        "]\n",
        "\n",
        "def preprocess_and_count(text_list):\n",
        "    # Combine all articles into one large string\n",
        "    combined_text = \" \".join(text_list).lower()\n",
        "\n",
        "    # Use Regex to remove punctuation and keep only alphanumeric words\n",
        "    words = re.findall(r'\\b\\w+\\b', combined_text)\n",
        "\n",
        "    # Define basic stop words to filter out \"noise\"\n",
        "    stop_words = {'the', 'is', 'and', 'in', 'to', 'of', 'are', 'this', 'were', 'for', 'with'}\n",
        "\n",
        "    # Filtered word list\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Calculate frequencies\n",
        "    word_counts = Counter(filtered_words)\n",
        "    return word_counts\n",
        "\n",
        "# Execute the analysis\n",
        "frequencies = preprocess_and_count(news_articles)\n",
        "\n",
        "# Identify the Top 5\n",
        "top_5 = frequencies.most_common(5)\n",
        "\n",
        "# 3. Display Results\n",
        "print(\"Full Word Frequencies:\")\n",
        "print(dict(frequencies))\n",
        "print(\"\\n--- TOP 5 TRENDING WORDS ---\")\n",
        "for word, count in top_5:\n",
        "    print(f\"{word.capitalize()}: {count} times\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_oOl78CNjnU",
        "outputId": "f38f6c05-23aa-4552-e4d6-9a6860cf905f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Word Frequencies:\n",
            "{'election': 4, 'results': 1, 'coming': 1, 'today': 1, 'close': 1, 'market': 4, 'trends': 1, 'show': 1, 'a': 1, 'shift': 1, 'technology': 3, 'stocks': 1, 'investors': 1, 'watch': 1, 'new': 2, 'policy': 2, 'changes': 1, 'affecting': 1, 'sector': 1, 'announced': 1, 'driving': 1, 'heights': 1, 'during': 1, 'cycle': 1, 'economic': 1, 'main': 1, 'focuses': 1}\n",
            "\n",
            "--- TOP 5 TRENDING WORDS ---\n",
            "Election: 4 times\n",
            "Market: 4 times\n",
            "Technology: 3 times\n",
            "New: 2 times\n",
            "Policy: 2 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Modern Approach & Alternatives\n",
        "\n",
        "While manual counting is great for EDA, modern industry applications use more robust tools:\n",
        "\n",
        "* **SpaCy / NLTK:** These libraries have built-in \"Stop Word\" lists for multiple languages and can handle **Lemmatization** (grouping \"running\", \"ran\", and \"runs\" into the single word \"run\").\n",
        "* **Word Clouds:** A visual alternative where the size of the word represents its frequency.\n",
        "* **Topic Modeling (LDA):** Instead of just counting words, Latent Dirichlet Allocation (LDA) can automatically group words into themes (e.g., it would recognize that \"stocks,\" \"investors,\" and \"market\" all belong to a \"Finance\" topic).\n",
        "\n",
        "\n",
        "## 4. Summary Table\n",
        "\n",
        "| Method | Best For | Pros | Cons |\n",
        "| --- | --- | --- | --- |\n",
        "| **Simple Counter** | Quick EDA | Fast, no dependencies | High noise (stop words) |\n",
        "| **NLTK/SpaCy** | Production Pipelines | Highly accurate, handles grammar | Slower, more memory |\n",
        "| **LDA / BERT** | Complex Trends | Discovers hidden themes | Requires high compute |\n"
      ],
      "metadata": {
        "id": "nZs8dFCHNSd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4. The Task: Unigram and Bigram Probability Modeling\n",
        "\n",
        "> **Question:** A chatbot team is building a component to predict the next word in a sentence. Using a corpus containing:\n",
        "> * “Natural language processing is interesting”\n",
        "> * “Natural language processing is useful”\n",
        ">\n",
        ">\n",
        "> **A)** Construct a **Unigram model** by calculating the probability of each word.\n",
        "> **B)** Construct a **Bigram model** by calculating the conditional probability of a word given the previous word.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Understanding the Concept\n",
        "\n",
        "In a **Unigram** model, we assume each word is independent. The probability of a word is simply its frequency divided by the total number of words.\n",
        "\n",
        "In a **Bigram** model, we assume the probability of a word depends *only* on the word immediately preceding it (this is known as a **Markov Assumption**).\n",
        "\n",
        "The conditional probability for a Bigram  is calculated as:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Implementation: Probability Calculations"
      ],
      "metadata": {
        "id": "mY49Qt9SOpmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "\n",
        "# 1. Prepare the Corpus\n",
        "corpus = [\n",
        "    \"Natural language processing is interesting\",\n",
        "    \"Natural language processing is useful\"\n",
        "]\n",
        "\n",
        "# Tokenize (simplistic approach for this example)\n",
        "tokens = []\n",
        "for sentence in corpus:\n",
        "    tokens.extend(sentence.split())\n",
        "\n",
        "total_words = len(tokens)\n",
        "\n",
        "# --- A) Unigram Model ---\n",
        "unigram_counts = Counter(tokens)\n",
        "unigram_probs = {word: count / total_words for word, count in unigram_counts.items()}\n",
        "\n",
        "# --- B) Bigram Model ---\n",
        "bigrams = []\n",
        "for sentence in corpus:\n",
        "    words = sentence.split()\n",
        "    # Create pairs of consecutive words\n",
        "    bigrams.extend(zip(words[:-1], words[1:]))\n",
        "\n",
        "bigram_counts = Counter(bigrams)\n",
        "# Dictionary to store P(w_n | w_{n-1})\n",
        "bigram_probs = defaultdict(dict)\n",
        "\n",
        "for (w1, w2), count in bigram_counts.items():\n",
        "    # Probability = Count(w1, w2) / Count(w1)\n",
        "    bigram_probs[w1][w2] = count / unigram_counts[w1]\n",
        "\n",
        "# 3. Display Results\n",
        "print(\"--- Unigram Probabilities ---\")\n",
        "for word, prob in unigram_probs.items():\n",
        "    print(f\"P('{word}'): {prob:.2f}\")\n",
        "\n",
        "print(\"\\n--- Bigram Conditional Probabilities ---\")\n",
        "for w1, next_words in bigram_probs.items():\n",
        "    for w2, prob in next_words.items():\n",
        "        print(f\"P('{w2}' | '{w1}'): {prob:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjxIkmFwOjaW",
        "outputId": "012263be-c9ff-41fa-a7cb-d3cde7228414"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Unigram Probabilities ---\n",
            "P('Natural'): 0.20\n",
            "P('language'): 0.20\n",
            "P('processing'): 0.20\n",
            "P('is'): 0.20\n",
            "P('interesting'): 0.10\n",
            "P('useful'): 0.10\n",
            "\n",
            "--- Bigram Conditional Probabilities ---\n",
            "P('language' | 'Natural'): 1.00\n",
            "P('processing' | 'language'): 1.00\n",
            "P('is' | 'processing'): 1.00\n",
            "P('interesting' | 'is'): 0.50\n",
            "P('useful' | 'is'): 0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bigrams)\n",
        "bigram_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRY4PAx6S3SV",
        "outputId": "751aa289-1b2f-42ac-ac6b-1c429de53496"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'interesting'), ('Natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'useful')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({('Natural', 'language'): 2,\n",
              "         ('language', 'processing'): 2,\n",
              "         ('processing', 'is'): 2,\n",
              "         ('is', 'interesting'): 1,\n",
              "         ('is', 'useful'): 1})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01eb6716",
        "outputId": "9d9d63be-03aa-4bb8-ef99-adf98f2961cb"
      },
      "source": [
        "a = \"Natural language processing is interesting\"\n",
        "a = a.split()\n",
        "\n",
        "# Get the zip object\n",
        "zipped_pairs = zip(a[:-1], a[1:])\n",
        "\n",
        "print(\"Iterating through the zip object:\")\n",
        "for pair in zipped_pairs:\n",
        "    print(pair)\n",
        "\n",
        "# Note: A zip object can only be iterated over once.\n",
        "# If you try to iterate again, it will be empty.\n",
        "# To see it again, you'd need to recreate the zip object:\n",
        "# list_of_pairs = list(zip(a[:-1], a[1:]))\n",
        "# print(\"\\nConverting to a list (recreating zip object):\")\n",
        "# print(list_of_pairs)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterating through the zip object:\n",
            "('Natural', 'language')\n",
            "('language', 'processing')\n",
            "('processing', 'is')\n",
            "('is', 'interesting')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Results Analysis\n",
        "\n",
        "### Unigram Results:\n",
        "\n",
        "Since \"Natural\", \"language\", \"processing\", and \"is\" appear twice, and \"interesting\" and \"useful\" appear once in a 10-word corpus:\n",
        "\n",
        "* **P(Natural):**\n",
        "* **P(interesting):**\n",
        "\n",
        "### Bigram Results:\n",
        "\n",
        "The Bigram model shows us the \"flow\" of the sentence:\n",
        "\n",
        "* **P(language | Natural):**  (Every time \"Natural\" appeared, \"language\" followed it).\n",
        "* **P(interesting | is):**  (After the word \"is\", there is a 50% chance the next word is \"interesting\").\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Modern Approach & Efficiency\n",
        "\n",
        "While N-grams are excellent for understanding text statistics, they struggle with the **\"Sparsity Problem\"**—if a chatbot encounters a word pair it hasn't seen before, the probability is zero.\n",
        "\n",
        "**Better Alternatives:**\n",
        "\n",
        "1. **Laplace Smoothing:** Adding 1 to all counts so we never have a 0% probability.\n",
        "2. **Kneser-Ney Smoothing:** A more sophisticated way to handle unseen words.\n",
        "3. **Neural Language Models (RNNs/LSTMs):** Instead of counting, these models use \"hidden states\" to remember long-term context beyond just the previous word.\n",
        "4. **Transformers (Attention):** Instead of looking only at the *previous* word, Transformers look at *every* word in the sentence simultaneously to predict the next one.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary Table\n",
        "\n",
        "| Model | Dependencies | Complexity | Context Window |\n",
        "| --- | --- | --- | --- |\n",
        "| **Unigram** | None | Extremely Low | 1 word |\n",
        "| **Bigram** | Previous 1 word | Low | 2 words |\n",
        "| **Trigram** | Previous 2 words | Medium | 3 words |\n",
        "| **Transformer** | All words | High | Thousands of words |\n"
      ],
      "metadata": {
        "id": "6xNQK_yHOY6P"
      }
    }
  ]
}